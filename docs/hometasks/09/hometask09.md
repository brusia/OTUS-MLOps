# Задание 9

## Общие положения

В рамках выполнения задания 9 при использовании библиотеки FastAPI реализовано REST API для взаимодействия с моделью. Код приложения предоставлен в директории

src/otus_mlops/fast_api/application.py

Т.к. исходные данные имели 2 целевых столбца (fraud и fraud_scenario), реализована возможность получать прогнозы по обоим столбцам (за это отвечают разные модели, один из которых - бинарный классификатор, вторая - мультиклассовый). Для этого в коде приложения предусмотрено 2 метода, возвращающие ответ на POST-запрос "/predict" и "/predict_scenario" для бинарной и мультиклассовой модели соответственно.

Настроен процесс сборки и публикации данного приложения с помощью github actions (после предварительной проверки собираемого образа при помощи линтинга).

Сервис разворачивается в kubernetes автоматически через helm chart на отдельной виртуальной машине с использованием minikube с помощью скрипта infra/main/scripts/kube_setup.sh, передаваемого в terraform.

По ссылкам ниже приведены скриншоты с результатами описанной работы.

- [kube_pod](docs/hometasks/09/kube_pod.png)

- [binary_fraud](docs/hometasks/09/requests/binary_fraud.png)
- [binary_simple](docs/hometasks/09/requests/binary_simple.png)
- [binary_simple_transaction](docs/hometasks/09/requests/binary_simple_transaction.png)

- [multiclass_fraud](docs/hometasks/09/requests/multiclass_fraud.png)
- [multiclass_simple](docs/hometasks/09/requests/multiclass_simple.png)

## Примечания

- В отличие от рекомендованного подхода, при котором модель изначально упакована в docker и требует обновления при каждом переобучение, реализован более универсальный подход, получающий модель из соотвествующего места на s3 (для использовании lakefs-s3-gateway мы имеем возможность подменять актуальную версию модели, оставляя путь lakefs-s3 одинаковым). Таким образом, исходный контейнер не требует обновления при обновления модели, но только в случае расширения возможностей приложения, добавления новых функций в API, изменений в структуре данных или внесения исправлений в текущую реализацию.
Альтернативой в данном случае было бы забирать модель из mlflow, но развёрнутом в настоящий момент реесте моделей хранятся spark-модели и, следовательно, этот подход предполагает предварительную конвертацию в onnx. В текущей реализации преобразование в onnx выполнено из sklearn фреймворка, что с одной стороны демонстрирует навыки и понимание необходимости и удобства использования универслаьного формата при выполнении inference, и с другой не связано с необходимостью повторно разворачивать spark-кластер для выполнения преобразований.
