# Задание 3

В рамках задания реализован скрипт для анализа данных (также разработана программная инфраструктура, поддерживающая возможное расширение для анализа произвольного набора данных, предусмотрено гибкое добавление анализаторов, есть возможность расширения для произвольного датасета), а также скрипт для очистки данных.

Предоставленный набор для анализа чрезвычайно велик для использования простейших инструментов и представления данных в виде pandas, в связи с чем основной датасет используется только для простейшего анализа (рассчёта базовых статистик -- cм. анализатор SparkBaseStatisticsDataAnalyser -- среднего значения, среднеквадратичного отклонения, минимального и максимального значений, медианы, 25 и 75 процентилей, коэффициент ассимметрии и риск "толстых хвостов"). Остальные анализаторы (на основе evidently, ruptures <!--  и statsmodels --> ) рассчитываются на основе подмножества предоставленного набор данных.

Для корректности анализа в качестве предварительной подготовки исходный набор данных разбивается на тестовый и репрезентативный в зависимости от даты (первые 70% данных считаюся reference-значениями, оставшиеся 30 - тестовыми). Из данных наборов впоследствии выполняется выбор для анализаторов.

Результаты анализа доступны в соответствующем бакете:
s3://brusia-bucket/data-analyse/

Очищенные данные в формате parquet доступны по ссылке:
s3://brusia-bucket/data/processed/

## Прочие комментарии

- По инфраструктуре: В ходе работы столкнулась с ограничением по доступным версиям python для предоставляемого облаком yandex cloud сервиса data processing (общалась с поддержкой, в ходе чего выяснилось, что версия data processing 2.2 распространяется с предустановленным python3.11, однако в этой версии кластер не установлен hdfs).

Использование старой версии интерпретатора существенно ограничивает возможности разработки: нет нормальной поддержки линтинга, статической проверки кода, pre-commitа и аннотации большинства современных библиотек (в том числе pandas, повсемество используемой для задач машинного обучения). Тем не менее для выполнения базовых задач курса предоставленная версия подходит.

Для расширения возможностей разработки имеет смысл самостоятельно развернуть hadoop-кластер (например, с использованием docker-compose). Поэкспериментирую, если останется свободное время между выполнением домашних заданий курса (или уже после его окончания).

- В текущей реализации дата-специфичные зависимости вынесены в константы. Для более гибкого использования возможно разширение текущей функциональности с последующим вынесением касмомизированных значений в виде параметров. Таким образом достигается более общая реализация универсального подхода к обработке и подготовке данных, а также гарантируется быстрое и удобное переиспользование для целого ряда подобных (однотипных) задач.
