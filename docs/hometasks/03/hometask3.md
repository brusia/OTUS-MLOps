# Задание 3

В рамках задания реализован скрипт для анализа данных (также разработана программная инфраструктура, поддерживающая возможное расширение для анализа произвольного набора данных, предусмотрено гибкое добавление анализаторов), а также скрипт для очистки данных.

Предоставленный набор для анализа чрезвычайно велик для использования простейших инструментов и представления данных в виде pandas, поэтому в качестве предварительного шага анализа на всех имеющихся данных для каждого признака производится рассчёт базовых статистик (вычисляются среднее значения, дисперсия, 25-ый и 75-ый квантили, коэффициент ассимметрии, риск "толстых хвостов", число дубликатов и пропущенных значений, а также строится гистограмма распределения), а также для всего набора строит матрица корреляции. Те же самые статистики расчитываются для двух приблизительно равных половин исходного набора данных (условное разделение на тестовый и целевой набор для последующего анализа смещения данных).

Остальные анализаторы (на основе evidently, ruptures и statsmodels) рассчитываются на основе сравнения гистограмм распределения признаков.

Результаты анализа доступны в соответствующем бакете:
s3://brusia-bucket/data-analyse/

Очищенные данные в формате parquet доступны по ссылке:
s3://brusia-bucket/data/processed/

## Прочие комментарии

- По инфраструктуре: В ходе работы столкнулась с ограничением по доступным версиям python для предоставляемого облаком yandex cloud сервиса data processing (общалась с поддержкой, в ходе чего выяснилось, что версия data processing 2.2 распространяется с предустановленным python3.11, однако в этой версии кластер не установлен hdfs).

Использование старой версии интерпретатора существенно ограничивает возможности разработки: нет нормальной поддержки линтинга, статической проверки кода, pre-commitа и аннотации большинства современных библиотек (в том числе pandas, повсемество используемой для задач машинного обучения). Тем не менее для выполнения базовых задач курса предоставленная версия подходит.

Для расширения возможностей разработки имеет смысл самостоятельно развернуть hadoop-кластер (например, с использованием docker-compose). Поэкспериментирую, если останется свободное время между выполнением домашних заданий курса (или уже после его окончания).

- По анализа данных: для чистоты исследования было бы также нелишним проводить анализ данных на произвольных выборках из общего датасета (и затем проводить анализ дрифта на таких выборках вместо гистограмм распределений, как следано в реализванном в ходу данной работы случае). В случае необходимости описанная функциональность может быть добавлена с минимальными изменениями в реализованной инфраструктуре фреймворка анализа и очистки данных.
