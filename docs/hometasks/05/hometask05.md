# Задание 5

В ходе выполнения предыдущих заданий и упражнений с yandex-cloud платформой столкнулась с ограничением на количество выдаваемых промокодов для yandex-cloud-а. Ввиду этого предлагаемый ранее подход пришлось пересмотреть: запускаемый скрипт для очистки данных теперь не проводит всестороннего анализа датасета, сравнения значений статистик с распределениями предыдущего дня и т.д., но удаляет строки, содержащие пропуски в данных и сохраняет преобразованные таким образом данные в формате parquet на s3.

В рамках выполнения задания 5 этот упрощённый скрипт для очистки данных запускается автоматически по расписанию согласно правилам, описанным ниже.

С помощью модульного подхода реализовано управление инфраструктурой.

В yandex-cloud всегда активна управляющая виртуальная машина, а также облачное хранилище. На виртуальной машине развёнут apache airflow сервис (поднимается из docker-compose, т.к. у yandex-cloud managed service были проблемы с предоставлением стабильного сервиса. Кроме того, ограничение на количество выдаваемых промокодов для yandex-cloud не позволяет развернуть airflow из инфраструктуры yandex-cloud-а и оставить его работать на несколько дней в виду исключительной дороговизны данного сервиса). По установленному расписанию (ежедневно) apache-airflow действует в соответствии с шагами pipeline-а:

1. Создаёт hadoop-кластер и копирует на него сырые данные из облачного хранилища (создание происхоит с использованием terraform)
Эта задача всегда завершается с ошибкой (однако кластер создаётся и оказывается доступен для работы)

2. Запускается python-оператор, который вытаскивает cluster-id созданного dataproc-ресурса, потому что SDK airflow ожидает именно cluster-id, а вовсе не cluster-name, обращение по которому доступно только из CLI.

3. Запускает скрипт для очистки данных (эмулируется последовательное поступление данных (сгруппированы по суткам)). После сохранения данных за "новой" день, последняя обработванная дата сохраняется для дальнейшего использования (в следующий раз скрипт запустится на новой дате, а не будет каждый раз запускаться на одних и тех же данных)

4. После завершения шага 3 неиспользуемые ресурсы (hadoop-кластер) удаляется (с помощью запуска соответствующего метода airflow SDK)

5. Страховочный шаг, испольняемый только в случае, когда 4 не отработал (был необходим до того, как я научиалсь получать cluster-id из XCOM, потому что DataprocDeleteClusterOperator также требует cluster-id и не воспринимает cluster-name; и остался в DAG-е как рудимент). Он запускает bash-оператор, который удаляет созданный кластер (используя cluster-name) с помощью утилиты командной строки yc.

В ходе выполнения задания управляющие скрипты terraform также претерпели изменения: основная директория infra содержит 2 модуля: storage и управляющий proxy.
При разворачивании proxy в качестве стартового скрипта по-прежнему передаётся infra/scripts/proxy_setup.sh, однако теперь он автоматичсеки разворачивает airflow сервис на виртуальной машине, а также запускает работу по расписанию.

Вся работа с hadoop производится из-под управления apache-airflow.

Результат выполнения скрипта очистки данных, запускаемых на hadoop-кластере по расписанию из apache airflow доступен по ссылке:

s3://brusia-bucket/data/processed/scheduled/

# Примечания

- Так как управляющая proxy-машина, развёрнутая в yandex-облаке прерываемая (это также сделано для экономии расходуемых средств промокода), для корректной работы описанной системы необходимо ежедневно перезапускать её мануально. За пределами игрушечного проекта этих ограничений не существует.

- Для настройки Apache Airflow потребовалось (также единожды во время развёртывания docker compose) мануально задать переменные окружения, используемые при запуске DAG'ов, а также создать подключение к Yandex Cloud Storage, используя WebUI интерфейс Airflow (т.к. для выбранной версии Аirflow 3.0.3 запрещён прямой доступ через ORM).

- Честно говоря, пока использование Airflow выглядит довольно громоздко (и немного костыльно). Запуск какого-то скрипта с s3 выглядит сырым подходом (опять же, версионирование при этом условное, только если над объектным хранилищем существует какая-то обвязка вроде lakeFS). В этом смысле контейнерный подход, при котором всё окружение со всеми требуемыми зависимостями изначально упаковывается в контейнер, и затем запускается в полностью контролируемой среде представляется более зрелым.
