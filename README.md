# OTUS-MLOps

Данный репозиторий создан для выполнения домашних заданий по курсу MLOps платформы обучения OTUS.

<!-- Start of Hometask 6 block -->

## Задание 6

### Описание изменений

К реализованной в рамках предыдущего задания (hometask 5) инфраструктуре (поднимаемой при помощи docker-compose на отдельной виртуальной машине managing_proxy) добавлены модули для работы с базой данных PostgreSQL и модуль с MLFlow сервером. Реализация из одного docker-compose файла использована с целью экономии средств на истекающих (и израсходованных при выполнении предыдущих заданий промокодов). docker-compose использует максимальное абстрагирование (все управляющие параметры вынесены в переменные окружения), что позволяет легко масштабировать систему на нескольких виртуальных машин (разделить сервисы на разные машины и определив необходимые переменные окружения на каждой из них).

Так как логика работы с каждый заданием немного меняется, реализованный ранее фреймворк требует существенных изменений для адаптации к новым требованиям. В текущем задании решено было использовать предоставленный скрипт otus-practice-regular-retrain/src/train.py в качестве образца (так, как будто он был предоставлен ML-командой) с незначительными изменениями, и большую часть работы сосредоточить на автоматизации и настройке инфраструктуры.

Так, над предоставленным кодом были совершены следующие действия: создан файл зависимостей pyproject.toml (автоматически при добавлении пакетов с uv), сам скрипт вместе с зависимостями упаковамы в Python пакет, который можно установить через pip (или другой пакетный менеджер). Для этого используется uv build (классический setup.py под капотом).

По-хорошему для хранения python-пакетов развёрнуть PyPI, куда и выкладывать их после сборки. В текущей реализации вместо индекса местом хранения wheels выступает объектное хранилище.

<!-- Для хранения python-пакетов развёрнут PyPI https://brusia.github.io/github-hosted-pypi, но такая реализация не поддерживает возможность публикации пакетов непосредственно в репозиторий из-за ограничений github pages, насколько я поняла.
куда и выкладываются пакеты после сборки. -->

Сохранённые модели доступны по ссылке:

s3://brusia-bucket/models/fraud_detection

Airflow DAG состоит из следующих шагов:

1. Создание dataproc-кластера для обучения модели.
В отличие от предыдущего задания, при создании кластера используется скрипт установки ({current_repo}/infra/main/scripts/prepare_cluster.sh -- он синхронизируется с облачным хранилищем s3 при каждом push-e в основную ветку разработки main средствами ci). В этом скрипте после создания кластера устанавливается необходимый пакет, содержащий наш скрипт.

2. Мы по-прежнему получаем cluster_id из предыдущей задачи.

3. Запуск скрипта обучения происходит из этого предустановленного модуля на pyspark-кластере так, как это и было задумано при выполнении домашнего задания 3.

4. После завершения обучения модели и логирования результатов обучения в mlflow, кластер удаляется.

5. yc-страховка, которая ещё раз попробует удалить кластер (уже средствами yc CLI), если предыдущая задача по какой-то причиине завершилась с ошибкой.
Такая страховка отчасти спасла, когда прерываемая VM (на которой у нас всё развёрнуто) прервала свою работу в момент исполнения DAG-а по очистке датасета. При возвращении работоспособности виртуальной машины последний шаг отработал успешно и удалил кластер.

[Скрин прилагаю](docs/hometasks/06/yc_delete_cluster.png)

Этот шаг также хорошо помогал при отладке, когда initial script при создании кластера отрабатывал с ошибкой -- в этом случае удаление созданного (и отправленного в "Dead"-state кластера) также происходит автоматически (т.к. шаг 4 в описанном случае завершается с ошибкой).

[Screen debug](docs/hometasks/06/yc_delete_if_dead.png)

Однако от логики initial script-а пришлось отказаться в пользу сборки готового venv-а для spark-worker-а.

### Граф исполнения airflow

[Model Train DAG](docs/hometasks/06/model_train_dag.png)

### WEB-интерфейс MLFlow

- [Experiments](docs/hometasks/06/mlflow_experiments.png)
- [Models](docs/hometasks/06/mlflow_models.png)
- [Runs](docs/hometasks/06/mlflow_runs.png)

### Примечания

Разворачивание всей инфраструктуры c установкой на VM docker и docker-compose, а также запуском всех описанных сервисов (airflow, mlflow, postress и вспомогательных сервисов) происходит при создании VM из конфигурации terraform и infra/main/scripts/proxy_setup.sh, запускаемого автоматически при создании VM.

Из оставшихся мануальных действий - необходимость задавать переменные окружения для Airflow. В планах расширить функциональность для автоматизации заполнения этих значений.

Исправлена наблюдаемая ранее ошибка, по которой ранее не сохранялись модели spark (а выгружались только _SUCCESS-файлы) - воспроизвелось повторно при выполнении текущего задания. Проблема локализуется при попытке сохранять модели в файловую систему master-ноды кластера, а затем оттуда выгружать в объектное хранилище. При использовании средств mlflow.spark для работы с объектным хранилищем модели сохраняются корректно. Альтернативный вариант решения - использованием распределённой файловой системы hdfs при работе с данными в spark-кластерах.

### Ограничеия текущего подхода

В текущей реализации последовательных домашних заданий предполагается, что созданные графы выполнения (например, реализованные на данный момент для очистки данных и обучения модели) запускаются и работают параллельно. В реальности необходимо учитывать ограничения, налагаемые облачной инфраструктурой и её тарифами относительно мощностей ресурсов, которые могут быть выделы одновременно в рамках текущего тарифа. Так, например, yandex-cloud не позволяет создать 2 независимо работающих hadoop-кластера используемой нами в рамках курса мощности из-за того, что есть ограничение на общее число предоставляемого ssd ресурса.

При проектировании реальных систем данные ограничения также необходимо учитывать (закладывать в бюджет, предусматривать расширение тарифного плана и т.д.).

<!-- End of Hometask 6 block. -->

## Oбщие положения

Для экономии места выполненные (и принятые) задания перемещены из главного файла README.md текущего репозитория и скрыты в соответствующих каталогах (docs/hometasks/<task_number>/hometask<task_number>.md)

Актуальное задание (для проверки) будет размещено в текущей версии README.md
Список предыдущих заданий

[Hometask 1](docs/hometasks//01/hometask1.md)
[Hometask 2](docs/hometasks/02/hometask2.md)
[Hometask 3](docs/hometasks/03/hometask3.md)
[Hometask 5](docs/hometasks/05/hometask5.md)
[Hometask 6](docs/hometasks/06/hometask06.md)
