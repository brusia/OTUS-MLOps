# OTUS-MLOps

Данный репозиторий создан для выполнения домашних заданий по курсу MLOps платформы обучения OTUS.

<!-- Start of Hometask 5 block -->

## Задание 5

### Общие положения

В ходе выполнения предыдущих заданий и упражнений с yandex-cloud платформой столкнулась с ограничением на количество выдаваемых промокодов для yandex-cloud-а. Ввиду этого предлагаемый ранее подход пришлось пересмотреть: запускаемый скрипт для очистки данных теперь не проводит всестороннего анализа датасета, сравнения значений статистик с распределениями предыдущего дня и т.д., но удаляет строки, содержащие пропуски в данных и сохраняет преобразованные таким образом данные в формате parquet на s3.

В рамках выполнения задания 5 этот упрощённый скрипт для очистки данных запускается автоматически по расписанию согласно правилам, описанным ниже.

С помощью модульного подхода реализовано управление инфраструктурой.

В yandex-cloud всегда активна управляющая виртуальная машина, а также облачное хранилище. На виртуальной машине развёнут apache airflow сервис (поднимается из docker-compose, т.к. у yandex-cloud managed service были проблемы с предоставлением стабильного сервиса. Кроме того, ограничение на количество выдаваемых промокодов для yandex-cloud не позволяет развернуть airflow из инфраструктуры yandex-cloud-а и оставить его работать на несколько дней в виду исключительной дороговизны данного сервиса). По установленному расписанию (ежедневно) apache-airflow действует в соответствии с шагами pipeline-а:

1. Создаёт hadoop-кластер из airflow SDK (менее гибкий подход, чем terraform).
Эта задача всегда завершается с ошибкой (однако кластер создаётся и оказывается доступен для работы)

2. Запускается python-оператор, который вытаскивает cluster-id созданного dataproc-ресурса, потому что SDK airflow ожидает именно cluster-id, а вовсе не cluster-name, обращение по которому доступно только из CLI.

3. Запускает скрипт для очистки данных (эмулируется последовательное поступление данных (сгруппированы по файлам из исходного предоставленного датасета)). В данном случае игрушечный пример: скрипт анализирует последнюю дату, сохранённую в результате препроцесинга, и из имеющихся "сырых" данных берёт на обработку следующий файл.
Таким образом при следующем запуска дага скрипт запустится на новом *.txt файле, а не будет каждый раз запускаться на одних и тех же данных.

4. После завершения шага 3 неиспользуемые ресурсы (hadoop-кластер) удаляется (с помощью запуска соответствующего метода airflow SDK)

5. Страховочный шаг, испольняемый только в случае, когда 4 не отработал (изначально был необходим до того, как я научиалсь получать cluster-id из XCOM, потому что DataprocDeleteClusterOperator также требует cluster-id и не воспринимает cluster-name; в итогой версии остался как рудимент). Он запускает bash-оператор, который удаляет созданный кластер (используя cluster-name) с помощью утилиты командной строки yc (overhead в данном случае - принудительная установка yc утилиты, что в смсыле накладных расходов существенно ниже, чем неудалённый и простаивающий dataproc-кластер).

### Описанный граф

[Data preprocess DAG](docs/hometasks/05/data_preprocess_DAG.png)

Задача create_cluster завершается с ошибкой, но в yandex cloud console мы видим, что кластер создан. Последующие задачи обработки данные, запускаемые на этом кластере, подтверждают, что создание завершилось корректно.

[YC Concole screenshot](docs/hometasks/05/yandex_cloud_console_clusters.png)

Ежедневное расписание:
[Schedule DAG screenshot](docs/hometasks/05/airflow_dag_schkeduled.png)

### Изменения и результаты

В ходе выполнения задания управляющие скрипты terraform также претерпели изменения: основная директория infra содержит 2 модуля: storage и управляющий proxy.
При разворачивании proxy в качестве стартового скрипта по-прежнему передаётся infra/scripts/proxy_setup.sh, однако теперь он автоматичеcки разворачивает airflow сервис на виртуальной машине, а также запускает работу по расписанию.

Вся работа с hadoop производится из-под управления apache-airflow, в связи с чем от модуля dataroc основной инфраструктуры пришлось избавить (с сожалением :))ю

Результат выполнения скрипта очистки данных, запускаемых на hadoop-кластере по расписанию из apache airflow доступен по ссылке:

s3://brusia-bucket/data/processed_with_airflow/

Логи Airflow доступны по ссылке:

s3://brusia-bucket/logs/airflow/

### Примечания

- Так как управляющая proxy-машина, развёрнутая в yandex-облаке прерываемая (это также сделано для экономии расходуемых средств промокода), для корректной работы описанной системы необходимо ежедневно перезапускать её мануально. За пределами игрушечного проекта этих ограничений не существует.

- Касательно расписания: первые три запуска выдержаны с периодичностью 1 раз в сутки, как это требовалось при постановки задачи. Спустя 3 суток периодичной изменена на запуск скрипта каждый 2 часа -- так как обработка ведётся только на 1 файле (последовательно шаг за шагом с каждым новым прогоном DAG-а записываются новые данные, как это было бы при работе в реальном времени), этого интервала вролне достаточно для препроцессинга данных, а также накладных расходов на создание и удаление кластера. Оставшиеся данные будут обработаны в ускоренном темпе исключительно с целью ускорить обработку всего датасета.

- Для настройки Apache Airflow потребовалось (также единожды во время развёртывания docker compose) мануально задать переменные окружения, используемые при запуске DAG'ов, а также создать подключение к Yandex Cloud Storage, используя WebUI интерфейс Airflow (т.к. для выбранной версии Аirflow 3.0.3 запрещён прямой доступ через ORM).

- Честно говоря, пока использование Airflow выглядит довольно громоздко (и немного костыльно). Запуск какого-то скрипта с s3 выглядит сырым подходом (опять же, версионирование при этом условное, только если над объектным хранилищем существует какая-то обвязка вроде lakeFS). Управление зависимостями и версиями пакетов на создаваемых облачных ресурсах для меня пока остаётся загадочным. В этом смысле контейнерный подход, при котором всё окружение со всеми требуемыми зависимостями изначально упаковывается в контейнер, и затем запускается в полностью контролируемой среде представляется более зрелым, простым и удобным.

<!-- End of Hometask 5 block. -->

## Oбщие положения

Для экономии места выполненные (и принятые) задания перемещены из главного файла README.md текущего репозитория и скрыты в соответствующих каталогах (docs/hometasks/<task_number>/hometask<task_number>.md)

Актуальное задание (для проверки) будет размещено в текущей версии README.md
Список предыдущих заданий

[Hometask 1](docs/hometasks//01/hometask1.md)
[Hometask 2](docs/hometasks/02/hometask2.md)
[Hometask 3](docs/hometasks/03/hometask3.md)
[Hometask 5](docs/hometasks/05/hometask5.md)
